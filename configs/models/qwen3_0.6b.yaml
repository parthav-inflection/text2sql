name: "Qwen3-0.6B"
model_path: "Qwen/Qwen3-0.6B"  # New Qwen3-0.6B model
model_type: "vllm"
generation_config:
  temperature: 0.0
  max_tokens: 32768
  top_p: 0.95
  top_k: 20
vllm_config:
  tensor_parallel_size: 1
  max_model_len: 4096  # Smaller context for CPU efficiency
  trust_remote_code: true
  device: "cpu"  # Force CPU inference
  enforce_eager: true  # Required for CPU inference
  disable_log_stats: true  # Reduce logging overhead
  block_size: 16  # Smaller block size for CPU
  swap_space: 4  # GB of swap space for CPU inference 