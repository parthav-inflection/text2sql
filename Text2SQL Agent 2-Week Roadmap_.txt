A Fortnight Sprint to a State-of-the-Art Text-to-SQL Agent: An Implementation Roadmap




Introduction: Synthesizing the Vanguard of Text-to-SQL Research


This document provides a comprehensive, day-by-day implementation plan for building a production-grade Text-to-SQL agent. It synthesizes the highest-impact techniques from recent seminal research to create a modular, robust, and high-performance system. The focus is on bridging the gap between academic benchmarks and the complex, messy reality of enterprise databases, which are often characterized by large schemas, ambiguous documentation, and diverse SQL dialects.1
The core challenge in modern Text-to-SQL development is that state-of-the-art performance requires more than a single Large Language Model (LLM) call. It demands a sophisticated, multi-stage agentic workflow that can reason over vast and poorly documented schemas, self-correct its own errors, intelligently resolve ambiguity, and operate efficiently at scale. This report details the construction of such a workflow, drawing upon a curated set of advanced techniques that address the full lifecycle of a query, from pre-computation and knowledge enrichment to runtime generation, selection, and high-performance serving.
This roadmap is designed for a senior technical team tasked with developing a cutting-edge Text-to-SQL solution. By the end of the two-week sprint detailed herein, the team will have a functional agent that incorporates best-in-class techniques for automated metadata enrichment, enterprise-scale schema handling, diverse candidate generation, robust candidate selection, and production-ready inference serving. The final product will be an agent that is not only accurate but also practical, scalable, and grounded in the most current and impactful research in the field.


Section 1: The Modular Agent Architecture: A Blueprint for Excellence


The proposed agent architecture is a cohesive system composed of four distinct but interconnected layers. This modular design ensures that components can be developed, tested, and upgraded independently, facilitating a staged implementation and long-term maintainability. The architecture moves from offline, pre-computation tasks that build a deep knowledge foundation to a series of runtime modules that handle query generation, refinement, and selection, all supported by a performance-oriented serving infrastructure.


1.1. The Foundation: The Automated Knowledge Layer (Pre-computation)


The objective of this foundational layer is to move beyond raw database schemas and enrich the agent's context with a deep, semantic understanding of the database before a query is ever received. This is a crucial pre-computation step that directly addresses the core problem identified by domain experts: "the most difficult part of query development lies in understanding the database contents".1 By investing in automated knowledge extraction upfront, the runtime task of generating SQL becomes significantly simpler and more accurate.
Component 1: Database Profiling & Summarization
This module systematically queries the database to extract empirical, data-driven properties. It goes beyond the schema definition to understand the data's actual content and structure. The process involves running automated queries to compute key statistics for each column, such as row counts, NULL value counts, the number of distinct values, minimum and maximum values, and a sample of the top-k most frequent values.1 Furthermore, to aid in the discovery of undocumented relationships, the module generates a minhash sketch for each column. These sketches allow for the rapid calculation of resemblance between column contents, providing strong signals for potential join keys that are not declared as foreign keys in the schema.1
The raw statistical output is then transformed into a more potent form of context. A dedicated "Profile Summarizer" sub-module formats the JSON profile of each column into a descriptive English paragraph. This paragraph is then fed to an LLM, which is prompted to generate both a "short" and a "long" semantic description. This process has proven remarkably effective, often uncovering hidden meanings and formats. For instance, it can correctly identify an ambiguously named CDSCode column as a County-District-School identifier by analyzing its format and the context of other columns in the table. It can also detect non-obvious data types, such as recognizing that a text field contains JSON-formatted data, an insight unavailable from the schema alone.1
Component 2: Query Log Mining
While profiling reveals what is in the data, query log mining reveals how the data is used by experts. This module parses historical SQL query logs—a repository of accumulated expertise—to extract invaluable, undocumented metadata. By analyzing queries written by Subject Matter Experts (SMEs), the system can automatically discover critical information that is often missing from formal documentation. This includes identifying complex, multi-field, or conditional join paths that are frequently used but not formally declared.1
The analysis of query logs can uncover a significant number of these practical join constraints; one study found that log analysis could identify 25% more useful equality constraints than were documented in the BIRD benchmark's schema.1 Beyond joins, this technique extracts named formulas and business logic embedded in queries, such as the calculation for
current_spend_and_planned from accounting_table.trans_amt + sum(planning_table.planned_amt). It also reveals common filter predicates and data transformations that constitute the unwritten rules of the database. This provides a scalable method for capturing "SME knowledge without the SME".1
Component 3: Documentation-to-Knowledge Graph (KG) Extraction
The final component of the knowledge layer processes unstructured documentation, such as markdown files, internal wikis, or PDFs, to build a formal, queryable knowledge graph. This is accomplished using an LLM-based pipeline, exemplified by KGGen, which performs a multi-stage extraction process.1 First, it performs entity extraction to identify key concepts (e.g., tables, columns, business terms). Second, it performs relation extraction to identify the subject-predicate-object triples that connect these entities.
A critical innovation in this process is the use of iterative entity and relation clustering. After an initial extraction, the LLM is prompted to identify and merge synonymous or closely related nodes and edges. For example, entities like "customer_id", "client_identifier", and the abbreviation "cust_id" would be clustered into a single canonical node. Similarly, predicates like "links to" and "is associated with" might be consolidated into a standard "references" edge.1 This clustering step is vital for creating a dense, non-redundant, and semantically coherent KG, which serves as a powerful, structured knowledge base about the database's conceptual model.


1.2. The Core Engine: Multi-Path Generation and Self-Refinement (Runtime)


The objective of the core engine is to generate a diverse and high-quality set of candidate SQL queries for a given natural language question. This stage operates at runtime and emphasizes reasoning diversity to maximize the probability that at least one correct query exists within the candidate pool. The quality of this candidate set is the primary determinant of the agent's final accuracy.1
Component 1: Enterprise Schema Compression
Real-world enterprise databases can contain hundreds or even thousands of tables and columns, making it impossible to include the full schema in an LLM's context window.1 To address this "long-context" problem, this component applies a two-stage compression strategy. First, it uses
pattern-based table grouping to identify and merge tables with similar naming conventions, which is common for time-sharded or partitioned data (e.g., GA_SESSIONS_20160801 through GA_SESSIONS_20170801). Only a single representative table's full schema is retained for the group, drastically reducing redundancy.1
For schemas that remain too large after grouping, a table-level LLM-based schema linker is employed. This module iterates through the remaining tables and uses an LLM to make a relevance judgment for each one based on the user's question. This approach effectively prunes the schema to only the most relevant tables, significantly reducing the prompt size while maintaining high recall of necessary information.1
Component 2: Multi-Path Candidate Generation
Rather than relying on a single prompt, this module utilizes multiple, distinct reasoning strategies in parallel to generate a diverse set of SQL candidates. This diversity is crucial, as different strategies excel at different types of questions. The key strategies include:
* Divide and Conquer (D&C) CoT: This method prompts the LLM to first decompose a complex question into smaller, manageable sub-problems. It then generates pseudo-SQL for each sub-problem and finally assembles these partial solutions into a complete, final query. This approach is particularly powerful for handling nested queries and queries with intricate WHERE or HAVING conditions.1
* Query Plan (QP) CoT: This strategy guides the LLM to think like a database engine. The prompt encourages the model to outline a step-by-step execution plan—identifying tables to scan, join operations, and filtering logic—before constructing the final SQL. This method excels at questions that require careful reasoning about the relationships between different parts of the question and the database schema.1
* Online Synthetic Example Generation: To help the model better understand the specific nuances of the target database, this technique generates tailored few-shot examples on-the-fly. These synthetic examples are created to be relevant to the user's question, incorporating the correct tables, columns, and required SQL features (e.g., GROUP BY, CASE statements), thereby providing highly specific guidance within the prompt.1
Component 3: Self-Refinement / Query Fixer
Each generated SQL candidate undergoes an iterative correction loop to ensure its validity. If a query fails during execution due to a syntax error, or if it executes but returns an empty result set, it is passed back to the LLM. The new prompt includes the faulty query along with the specific error message or feedback from the database. The LLM is then tasked with "self-debugging" or refining the query. This process is repeated for a fixed number of attempts (e.g., three), significantly increasing the likelihood of producing an executable and semantically plausible query.1


1.3. The Judgment and Exploration Layer: Consensus, Deferral, and Discovery (Runtime)


The objective of this layer is to intelligently select the single best query from the candidate pool generated by the core engine. It also includes a crucial mechanism for actively resolving ambiguity when no clear winner emerges, embodying the agent's advanced decision-making and exploration capabilities.
Component 1: Consensus Enforcement (Majority Voting)
This is the first and fastest method for candidate selection. The system executes all syntactically valid candidates and groups them based on their execution results. If a single result is produced by more candidates than any other result (a unique majority), its corresponding SQL query is selected as the high-confidence answer. This approach is highly effective for a large class of queries where the model is confident, and it serves as an efficient filter to resolve straightforward cases without invoking more expensive modules.1
Component 2: Deferral and Column Exploration
If the majority vote results in a tie, or if no clear consensus is reached, the query is considered "low-confidence" and is deferred for deeper analysis. At this point, the Iterative Column Exploration module is triggered. This module is a key innovation for handling ambiguity. Instead of trying to answer the question directly, it prompts the LLM to generate a series of simple, targeted SELECT queries to probe the database. These exploratory queries are designed to understand the content and structure of potentially relevant columns, especially those with nested formats like JSON or ambiguous value types. The agent executes these queries, collects the results in a "knowledge scratchpad," and uses this newly gathered, grounded information to make a second, more informed attempt at generating the final SQL query.1 This structured exploration is more reliable than unconstrained agentic frameworks, which can struggle to maintain control on complex tasks.1
Component 3: Advanced Selection (Trained Pairwise Agent)
As a more sophisticated alternative to majority voting, this component uses a fine-tuned binary classification model to act as a "selection agent." This model is trained specifically on the task of comparing two candidate SQL queries and predicting which one is more likely to be correct for a given question. To select the best from a pool of n candidates, the agent performs pairwise comparisons, and the candidate that "wins" the most comparisons is chosen as the final answer. This approach significantly outperforms simple self-consistency or majority voting, as it can learn the subtle nuances that distinguish a correct query from a nearly-correct one, proving especially valuable in scenarios without a clear majority.1


1.4. The Performance Backbone: A Workflow-Aware Serving Infrastructure (Production)


The objective of this final layer is to ensure that the entire multi-stage agentic workflow can be served in a production environment with high throughput and low latency, meeting strict Service Level Objectives (SLOs). This addresses the critical operational reality that agentic workflows, with their multiple interdependent LLM and database calls, are computationally expensive and present unique scheduling challenges.1
Component 1: Global, Workload-Balanced Dispatcher
A central coordinator manages the entire workflow for every incoming query. It receives all LLM inference requests generated by the various agent stages (e.g., schema linking, candidate generation, self-correction). It then intelligently dispatches each request to the most suitable model instance within a heterogeneous GPU cluster. The dispatching decision is based on a score that balances the target instance's raw processing capability (e.g., an A100 vs. an L40) against its current workload and queue length, preventing bottlenecks and ensuring efficient load distribution.1
Component 2: Local, Urgency-Guided Priority Queues
Each model instance manages its local request queue not by a simple First-Come-First-Served (FCFS) policy, but by "urgency." The urgency of a request is dynamically calculated based on the remaining time budget for the end-to-end query's SLO. A request that is part of a query nearing its deadline will have a higher urgency. This allows critical tasks to preempt less time-sensitive ones, drastically reducing the risk of SLO violations under heavy load.1
Component 3: Lightweight, Trace-Driven Simulator
To optimize the performance of the global dispatcher, a lightweight simulator runs on the CPU. It continuously replays recent request traces and simulates the system's performance under different settings of a key scheduling hyperparameter, α, which controls the trade-off between execution speed and load balancing. By finding the optimal α for the current workload pattern, the simulator allows the serving system to adaptively tune itself for maximum performance without interrupting live service.1
Component/Module
	Primary Technique
	Key Contribution
	Source Paper(s)
	Knowledge Layer
	Database Profiling & Summarization
	Turns raw stats into semantic column descriptions
	1
	

	Query Log Mining
	Extracts undocumented business logic and joins
	1
	

	Documentation-to-KG
	Creates a structured, queryable knowledge base from text
	1
	Generation Engine
	Enterprise Schema Compression
	Manages massive schemas via grouping and linking
	1
	

	Multi-Path Candidate Generation
	Improves accuracy via diverse reasoning (D&C, QP CoT)
	1
	

	Self-Refinement / Query Fixer
	Iteratively corrects syntax and execution errors
	1
	Judgment Layer
	Majority-Vote Consensus
	Fast, heuristic-based selection for high-confidence cases
	1
	

	Iterative Column Exploration
	Actively resolves ambiguity for low-confidence cases
	1
	

	Trained Pairwise Selection Agent
	Advanced, fine-tuned selection for superior accuracy
	1
	Serving Infrastructure
	Two-Level Scheduling
	Manages agentic workflows on heterogeneous hardware
	1
	

	Trace-Driven Simulation
	Optimizes scheduling hyperparameters for performance
	1
	

Section 2: The 14-Day Implementation Roadmap


This section provides a granular, day-by-day plan with specific tasks, goals, and "impressive deliverables" for each day. The roadmap is structured to build the system incrementally, starting with the foundational components and progressively adding more advanced capabilities, ensuring a functional and increasingly powerful agent is available at each stage of the two-week sprint.


Week 1: Building the Core Generation and Selection Pipeline


The first week is dedicated to constructing a robust, end-to-end pipeline that can ingest a question, generate SQL, and select a reliable answer. The focus is on implementing the core reasoning and correction mechanisms.
Day 1-2: The Knowledge Foundation: Database Profiling and Metadata Summarization
* Focus: The initial two days are dedicated to implementing the foundational offline knowledge extraction pipeline. This is the first and most critical step, as all subsequent runtime modules depend on this enriched metadata to function accurately and avoid hallucination. The quality of this layer directly impacts the ceiling of the agent's performance.
* Tasks:
   1. Develop scripts to perform basic database profiling. These scripts will connect to a target database, iterate through all tables and columns, and execute automated queries to compute essential statistics: row count, NULL count, number of distinct values, min/max values for numeric/date columns, and a sample of the top-k most frequent values for categorical columns.1
   2. Integrate a library like datasketch to generate a minhash sketch for the values in every column. These sketches will be stored alongside the other profile statistics and will serve as a pre-computed index for rapidly finding columns with high value overlap, a strong indicator of potential join keys.1 All extracted profiles should be saved in a structured format, such as one JSON file per table.
   3. Create a "Profile Summarizer" module. This module will take the structured JSON profile of a column, programmatically format it into a descriptive English paragraph (e.g., "Column CDSCode has 0 NULL values out of 9986 records..."), and then use an LLM (e.g., GPT-4o) to distill this technical summary into both a "short" and a "long" semantic description. The short description captures the core meaning, while the long description includes example values to guide the generation model.1
   4. As a stretch goal, begin the implementation of a query log parser using a robust SQL parsing library like sqlglot. The initial goal is to parse a log file of SQL queries and correctly extract all referenced tables, columns, and explicit join conditions (R.f=S.g).1
* Daily Deliverable (Day 2): A command-line tool that can be pointed at a database (e.g., a local SQLite file from the BIRD benchmark) and produces a folder of JSON files. Each file will correspond to a table and contain rich, LLM-generated semantic descriptions for every column. The deliverable should be demonstrated on a sample database, showing how the tool correctly infers the meaning of an ambiguously named column (e.g., frpm.CDSCode) and accurately describes its data format, proving the value of the enrichment process.
Day 3-4: The First Query: Basic Schema Linking, Value Retrieval, and Single-Path Generation
* Focus: With the knowledge foundation in place, the next two days focus on building the simplest possible end-to-end query pipeline. The goal is to successfully generate and execute a basic SQL query that answers a natural language question.
* Tasks:
   1. Implement a basic schema representation module. This module will be responsible for loading the database's DDL (from CREATE TABLE statements) and merging it with the newly generated profile summaries from Day 1-2 to create a comprehensive, enriched schema context.
   2. Implement a value retrieval module. This module will first use an LLM to extract key entities and literal values (e.g., "California", "September 2024") from the user's question. It will then use a technique like Locality-Sensitive Hashing (LSH) on pre-computed indexes of database values to find the most syntactically similar values in the database content. This step is crucial for grounding the query with correct literals in the WHERE clause.1
   3. Build the initial prompt constructor. This module will assemble the final prompt for the LLM by taking the user's question, the full (uncompressed) enriched schema representation, and any values retrieved by the value retrieval module.
   4. Create the single-path generation module. This is a straightforward wrapper around an LLM API call that takes the constructed prompt and generates a single SQL query as output.
* Daily Deliverable (Day 4): A working Python script that can take a natural language question for a small database, generate a single SQL query, and execute it against the database. The deliverable must successfully answer a simple question that requires both schema understanding and value retrieval, such as "How many customers are from 'Fresno County'?", demonstrating that the agent can link the literal 'Fresno County' to the correct column.
Day 5: Diversifying the Reasoning: Integrating Multi-Path Candidate Generation
* Focus: Transition from generating a single, potentially flawed query to producing a diverse set of candidates. This diversity is the raw material for all subsequent refinement and selection techniques and is a cornerstone of modern, high-performance agents.
* Tasks:
   1. Refactor the generation module from Day 4 to support multi-path generation. This involves creating a system that can invoke multiple, different prompt templates for the same user question.
   2. Implement the Divide and Conquer (D&C) CoT prompter. This will require designing a single, detailed prompt that instructs the LLM to first break the question into logical sub-questions, then generate pseudo-SQL for each part, and finally assemble them into a final, complete query. The implementation should follow the structure outlined in the CHASE-SQL paper.1
   3. Implement the Query Plan (QP) CoT prompter. This will involve creating a prompt that guides the LLM to think like a database engine, explicitly outlining the steps of table access, join operations, filtering, and aggregation before writing the final SQL.1
   4. Modify the main generation logic to run these different prompters in parallel (or sequentially) for a single user question, collecting all generated SQL candidates into a list. To further enhance diversity, introduce mechanisms for randomizing the order of tables and columns in the prompt and for using a higher sampling temperature (e.g., T=1.0) in the LLM API calls.1
* Daily Deliverable (Day 5): An updated script that, when given a single complex question (e.g., one requiring a subquery or join), generates and displays at least 3-5 different SQL candidates. The output should showcase the varied reasoning paths produced by the D&C CoT, QP CoT, and standard generation with prompt randomization.
Day 6-7: Ensuring Robustness: Implementing Self-Correction and Majority-Vote Consensus
* Focus: Build the first layer of judgment and correction. This will make the agent more robust by allowing it to fix its own mistakes and select the most reliable answer from its generated candidates.
* Tasks:
   1. Implement the Query Fixer / Self-Correction loop. For each candidate generated on Day 5, the agent will attempt to execute it against the database. If the execution fails, the database error message (e.g., syntax error, no such column) will be captured. The original query and the error message are then passed back to the LLM in a new prompt that explicitly asks it to fix the query. This process should be looped up to a specified number of retries (e.g., 3).1
   2. Implement the Majority-Vote Consensus module. After the self-correction step, take all candidates that are now syntactically valid. Execute each one and convert its result set into a canonical, hashable format (e.g., a sorted list of sorted tuples).
   3. Group the candidates by their canonical execution results and count the number of "votes" for each unique result.
   4. Implement the core selection logic: if one result has a unique majority (i.e., its vote count is strictly greater than any other), select its corresponding SQL as the final answer. If there is a tie for the highest vote count, flag the query as "low-confidence." For this week's deliverable, the agent can simply select one of the tied candidates at random.1
* Daily Deliverable (Day 7): A robust end-to-end agent that can take a complex question, generate multiple candidates, automatically fix common syntax errors, and select a final answer based on majority vote. The deliverable should be a live demonstration where the agent is given a query that initially produces some incorrect SQL. The logs should show the self-correction process in action, followed by the majority vote correctly identifying the right answer from the mix of corrected and initially-correct candidates.


Week 2: Advanced Capabilities and Production Readiness


The second week is focused on scaling the agent to handle enterprise-level complexity, implementing advanced ambiguity resolution, and designing the high-performance infrastructure required for production deployment.
Day 8-9: Taming Enterprise Scale: Implementing Database Information Compression
* Focus: Address the critical long-context problem to ensure the agent is viable for use with large, real-world enterprise databases that have hundreds or thousands of columns.
* Tasks:
   1. Implement the pattern-based table grouping module. This script will scan all table names in a given database and use regular expressions or simple string matching to identify groups of tables with similar prefixes or suffixes (e.g., _2023, _2024, _daily_summary). The schema representation logic will then be modified to only include the full DDL and enriched metadata for one representative table per group, with the others listed by name only.1
   2. Implement the table-level LLM-based schema linker. For schemas that are still too large after the grouping step (e.g., >50K tokens), this module will be triggered. It will iterate through each table (or table group), providing its name and enriched description to an LLM with a simple prompt like: "Based on the user's question, is this table likely to be relevant? Answer Y/N".1
   3. Integrate this two-stage compression logic into the main prompt constructor from Day 3. The prompt sent to the generation models will now contain a much smaller, more targeted, and more relevant schema, freeing up context space for reasoning.
* Daily Deliverable (Day 9): A successful demonstration of the agent answering a query on a database with a very large schema (e.g., using a public dataset like bigquery-public-data.noaa_hurricanes from the Spider 2.0 benchmark as a test case). The deliverable should include logs that clearly show the original schema size (in tokens) and the final, compressed schema size, proving the effectiveness and necessity of the compression pipeline.1
Day 10-11: Resolving Deep Ambiguity: Building the Iterative Column Exploration Module
* Focus: Implement the agent's most advanced and powerful reasoning capability: the ability to actively explore the database to resolve ambiguity when it is uncertain.
* Tasks:
   1. Modify the consensus module from Day 7. Instead of randomly selecting a candidate in the case of a tie, it should now trigger the Column Exploration module for all "low-confidence" examples.1
   2. Design the column exploration prompter. This prompt is different from the main generation prompt; it instructs the LLM to act as an explorer, generating a series of simple, targeted SELECT queries (e.g., SELECT DISTINCT col FROM tbl LIMIT 10, SELECT f.value FROM table, LATERAL FLATTEN(input => t.json_column) f) to investigate the contents of specific columns, especially those with nested or complex types like JSON or GEOGRAPHY.1
   3. Implement the exploration loop. The agent will execute these exploratory queries, capture their results, and append them to a "knowledge scratchpad" for the current session.
   4. Crucially, if an exploratory query fails, it should trigger the self-correction loop (from Day 6) to fix it, allowing the agent to learn about dialect-specific syntax for complex data types.
   5. After the exploration phase concludes, the agent will re-run the main multi-path generation pipeline (from Day 5). However, this time the prompt will be augmented with the new, grounded knowledge gathered in the scratchpad, providing the LLM with concrete examples of the data it needs to query.
* Daily Deliverable (Day 11): A compelling end-to-end demonstration of the full ReFORCE agent on a particularly ambiguous or complex query (e.g., one involving nested JSON fields). The deliverable should be a detailed log or trace that shows: 1) the initial multi-path generation fails to reach a consensus, resulting in a tie; 2) the column exploration module is triggered; 3) the series of exploratory queries and their results are displayed; and 4) the final, successful SQL generation that correctly uses the knowledge gained from the exploration to construct a complex query.
Day 12: Perfecting the Selection: Training and Integrating the Pairwise Comparison Selection Agent
* Focus: Implement the more sophisticated, trained selection agent as a high-accuracy alternative to simple majority voting, following the methodology from the CHASE-SQL paper.1
* Tasks:
   1. Data Generation: Set up a pipeline to run the multi-path generation module (from Day 5) over a large set of existing question-SQL pairs (e.g., the BIRD or Spider training sets). This will produce a large corpus of candidate queries for each training question.
   2. Automated Labeling: For each question in the training set, label the generated candidates as "correct" or "incorrect" by executing them and comparing their results to the known gold answer. From this, create a dataset of pairwise training examples of the form (question, query_A, query_B, label), where the label indicates whether A or B is the correct query.
   3. Fine-tuning: Use a framework like Vertex AI or Hugging Face's TRL to fine-tune an efficient model (e.g., Gemma, Llama3-8B, or Gemini 1.5 Flash) on this binary classification task. The goal is to create a specialized, low-latency model that excels at predicting which of two SQL queries is better for a given question.1
   4. Integration: Create a new selection module that replaces the majority-vote logic. For a set of n candidates, this module will run n*(n-1)/2 pairwise comparisons using the fine-tuned model. It will then tally the "wins" for each candidate and select the one with the most wins as the final answer.1
* Daily Deliverable (Day 12): A report, including tables and charts, that compares the accuracy of the original Majority-Vote selector against the new fine-tuned Pairwise Selection Agent on a held-out development set. The report should demonstrate that the trained agent significantly outperforms majority voting, especially on difficult questions where no clear consensus exists, justifying the added complexity of fine-tuning.1
Day 13-14: Designing for Speed: Architecting the High-Performance Serving System
* Focus: Design the production-ready serving infrastructure required to run the complete, multi-stage agentic workflow efficiently and reliably under load.
* Tasks:
   1. System Architecture Design: Using a diagramming tool, whiteboard the full HEXGEN-TEXT2SQL architecture. This diagram should clearly show the Global Coordinator, the multiple Model Instances (on heterogeneous hardware), and the flow of requests. Define the API contracts and data structures for communication between these components. The coordinator will be responsible for managing the state of each end-to-end query workflow.1
   2. Implement the Global Dispatcher: Write the core logic for the coordinator's workload-balanced dispatching policy. This module will need to maintain a model of the serving cluster's capabilities (e.g., a map of instance IDs to GPU types like A100, L40) and have a mechanism to receive real-time updates on the queue depths of each instance.1
   3. Implement the Local Priority Queue: In the model instance server (e.g., a custom server built on a framework like vLLM), replace the default FCFS queue with a priority queue. The priority of each request will be its "urgency," which is dynamically calculated based on its remaining SLO budget. This ensures that time-critical requests are processed first.1
   4. Implement the alpha-Tuning Simulator: Develop the lightweight, trace-driven simulator as a separate utility. This tool will take a log of requests as input, replay them against a simulated model of the cluster, and sweep through different values of the alpha hyperparameter to find the setting that minimizes average latency or maximizes SLO attainment. This is a crucial offline tool for performance tuning.1
* Daily Deliverable (Day 14): A complete architectural diagram of the proposed serving system and a proof-of-concept implementation of the global dispatcher. The deliverable should also include a simulation result (e.g., a graph) showing how the system with an optimized alpha value from the simulator significantly outperforms a naive round-robin dispatcher in terms of SLO attainment for a synthetic workload, proving the value of the workflow-aware scheduling design.


Section 3: Deep Dive: Analysis of Critical Design Choices


This section provides the underlying rationale and justification for the key architectural decisions made in the roadmap. It moves beyond what to build and explains why these specific choices are critical for achieving a state-of-the-art system, drawing connections between the empirical evidence in the research and the practical implementation plan.


3.1. From Schema to Knowledge: Why Automated, Rich Metadata is Non-Negotiable


A central principle of the proposed architecture is that the single greatest lever for improving Text-to-SQL accuracy in complex, real-world settings is the quality and richness of the metadata provided to the LLM. Simply providing raw DDL is insufficient and forces the model to guess, often incorrectly, about column semantics, data formats, and business rules. The system must instead operate on a higher level of abstraction, a "knowledge layer," that is built through automated processes.
This approach is directly supported by extensive empirical evidence. The work from AT&T, based on decades of industrial database experience, explicitly states that the primary difficulty in query development is not writing SQL, but understanding the database to begin with.1 Their experiments on the BIRD benchmark provide a powerful validation of this principle. When using LLM-summarized metadata generated from automated database profiling, their system achieved a higher accuracy (61.2%) than when using the official, human-supplied metadata provided with the benchmark (59.6%).1 This finding suggests that an automated, data-driven approach to metadata generation can surpass the quality of manual, and often incomplete or outdated, documentation.
This leads to a more nuanced view of the context provided to a Text-to-SQL agent, which can be seen as a hierarchy of knowledge. At the bottom is the raw schema (table and column names), which is often cryptic. The next level is human-written documentation, which, while helpful, can be inconsistent. A more powerful level is the dynamic, data-driven context derived from profiling, as it reflects the actual state of the data. Even higher is the behavioral context mined from query logs, which reveals how experts use the data, exposing implicit logic. At the top is a conceptual model of the domain, which can be extracted from unstructured text using techniques like knowledge graph construction.1 A state-of-the-art agent must be built to leverage the highest levels of this hierarchy. The roadmap therefore prioritizes the construction of this knowledge layer in the first week, as it provides the highest return on investment for improving the agent's core reasoning capabilities.


3.2. The Power of the Ensemble: Comparing Majority Voting with a Trained Selection Agent


The architecture proposes two primary methods for candidate selection: a simple, heuristic-based majority vote and a more complex, fine-tuned selection agent. The choice between them represents a fundamental trade-off between implementation simplicity, inference cost, and ultimate accuracy.
Majority voting, as employed by ReFoRCE, is a fast and effective heuristic.1 It relies on the principle of self-consistency: if multiple independent reasoning paths converge on the same answer, that answer is likely to be correct. Its primary advantages are its simplicity (it requires no model training) and its ability to quickly filter and confirm high-confidence answers. However, its significant drawback is its performance ceiling. As shown in the CHASE-SQL experiments, the upper-bound accuracy achievable from a pool of candidates can be substantially higher (up to 14 percentage points) than what self-consistency alone can achieve.1 This is because majority voting fails precisely when the question is most difficult—when there is no clear consensus among the candidates.
In contrast, a trained pairwise selection agent, as proposed in CHASE-SQL, is designed to excel in these high-ambiguity scenarios.1 By fine-tuning a model on the specific task of comparing two SQL queries, it learns the subtle semantic and syntactic patterns that differentiate a correct query from a nearly-correct one. While this approach requires an upfront investment in data generation and model fine-tuning, it offers two key advantages: a higher accuracy ceiling and lower inference cost
at selection time. The selection agent does not need to execute the queries against a database to make its decision; it operates purely on the text of the queries and the schema, making it much faster than executing k candidates for a majority vote. The table below summarizes the trade-offs.
Method
	Implementation Complexity
	Training Cost
	Inference Cost
	Accuracy Ceiling
	Best For
	Source Paper(s)
	Majority Voting
	Low (execute, hash results, count)
	None
	High (requires executing all k candidates)
	Medium (Fails on ambiguity)
	Initial MVP, high-confidence queries
	1
	Trained Pairwise Selector
	High (data generation, fine-tuning)
	Medium (requires one-time fine-tuning)
	Low (only LLM calls, no DB execution needed for selection)
	High (Can resolve ambiguity)
	Maximum accuracy, production systems
	1
	Ultimately, the choice of selection mechanism defines the agent's strategy for handling uncertainty. Majority voting is a deferral strategy; when faced with ambiguity (a tie), it punts the problem to a more expensive module like Column Exploration.1 The trained selector is a
resolution strategy; it is designed to make a definitive choice even in high-uncertainty situations. A novel, hybrid approach not explicitly detailed in any single paper could offer the best of both worlds: use majority voting as a fast-path filter. If a clear consensus is reached, the process stops. If not, invoke the cheaper pairwise selection agent. Only if the selection agent's top choice still fails would the system resort to the most expensive option, column exploration. This tiered strategy for managing uncertainty would create a highly efficient and accurate system.


3.3. Structured vs. Unconstrained Agency: The Case for ReFoRCE's Column Exploration


For a specialized and high-stakes task like Text-to-SQL, a structured, purpose-driven agentic workflow is superior to a general, unconstrained framework like ReAct. General-purpose coding agents, while flexible, often "struggle to maintain control," "fail to follow instructions," and "produce inconsistent and unreliable outputs" when applied to complex, domain-specific problems.1 The risk of error propagation and hallucination is high in an unconstrained loop.
The ReFoRCE framework provides a compelling alternative through its highly structured workflow. Instead of a single, open-ended "reason-act" loop, ReFoRCE decomposes the problem into distinct, controllable sub-tasks. The most innovative example of this is the Column Exploration module. This module is not a general-purpose tool; it is a specialized procedure that is triggered for a specific reason (low-confidence results from majority voting) and has a specific goal (to resolve ambiguity about column contents and structure). The case study in the ReFoRCE paper's appendix provides powerful evidence for this approach. When faced with a complex geospatial query, the agent initially fails repeatedly. However, once the structured column exploration is triggered, the agent systematically probes the database with simple queries, learns the correct dialect-specific syntax for handling nested data, and then uses this grounded knowledge to construct the correct final query.1 This demonstrates that for reliable enterprise applications, a structured workflow that constrains the agent's actions and guides its reasoning process is more effective than a more flexible but less predictable general agent.


3.4. Beyond the Algorithm: The Imperative of Workflow-Aware Inference Scheduling


Even the most sophisticated logical agent will fail to perform in a production environment without a serving system designed to handle its unique operational characteristics. Agentic Text-to-SQL workflows are fundamentally different from traditional, single-request LLM tasks. They represent a Directed Acyclic Graph (DAG) of interdependent LLM and database calls, where delays in early stages can have cascading effects on the ability to meet the end-to-end query's SLO.
The HEXGEN-TEXT2SQL paper provides the core argument and solution for this operational challenge.1 It demonstrates that naive scheduling policies like First-Come-First-Served (FCFS) or simple round-robin, which are common in general-purpose LLM serving systems, are ill-suited for these complex workflows. Such policies lead to "suboptimal resource utilization, frequent SLO violations, and degraded system responsiveness" because they are blind to task dependencies and urgency.1
The proposed two-level scheduling system directly addresses this. The global, workload-balanced dispatcher ensures that requests are sent to the most appropriate hardware, while the local, urgency-guided priority queue ensures that the most time-critical tasks are processed first. This workflow-aware approach yields dramatic performance improvements, with reported throughput gains of up to 1.75x and a significant reduction in deadline violations compared to standard serving systems.1 This is not a minor optimization; it is a fundamental architectural requirement for deploying a responsive and reliable Text-to-SQL agent in a real-world, multi-tenant setting. The logical correctness of the agent must be paired with an equally sophisticated serving infrastructure to deliver value in production.


Section 4: Strategic Recommendations and Future Trajectory


Building on the detailed technical roadmap and design analysis, this section provides strategic recommendations for deploying the Text-to-SQL agent and outlines promising directions for future development.


4.1. A Phased Deployment Strategy


A phased approach to deployment will allow for incremental value delivery and continuous feedback, reducing risk and aligning development with business needs.
* Phase 1 (MVP - End of Week 1): The initial deployment should focus on the core generation pipeline. This Minimum Viable Product will consist of the Automated Knowledge Layer (Database Profiling and Summarization), the Core Engine (Multi-Path Generation and Self-Refinement), and the Judgment Layer using simple Majority-Vote Consensus. This configuration provides a robust baseline agent that can handle a significant portion of common queries efficiently and accurately, delivering immediate value.
* Phase 2 (Advanced Capabilities - End of Week 2): The second phase focuses on integrating the advanced modules to handle enterprise-scale complexity and ambiguity. This includes deploying the Enterprise Schema Compression module to support large databases, activating the Iterative Column Exploration module to resolve low-confidence queries, and swapping the majority-vote selector for the fine-tuned Pairwise Selection Agent to maximize accuracy.
* Phase 3 (Production Hardening): The final phase is centered on operational excellence. The complete agent, with all its logical capabilities, should be deployed on the high-performance HEXGEN-TEXT2SQL serving infrastructure. This step is critical for ensuring the agent is not just smart but also fast and reliable enough for production workloads, capable of meeting stringent SLOs under concurrent user load.


4.2. A Holistic Evaluation Framework


To truly measure the success of the agent, evaluation must go beyond simple Execution Accuracy (EX) and encompass a more holistic set of metrics that reflect real-world performance and cost. The following framework is recommended:
* Quality Metrics:
   * Execution Accuracy (EX): The percentage of generated queries that execute and produce the correct final answer.
   * EX@k: The pass rate within the top-k generated candidates. This measures the effectiveness of the candidate generation module, indicating the upper bound of performance for the selection agent.
* Cost Metrics:
   * Average LLM Calls per Query: Measures the computational cost in terms of model invocations.
   * Average DB Calls per Query: Measures the load placed on the database, especially important for the column exploration module.
   * Average Tokens per Query: Tracks the total token count (prompt + completion) to monitor API costs.
* Speed & Performance Metrics:
   * End-to-End Latency: Measured at the 50th, 90th, and 99th percentiles to understand typical and worst-case user experience.
   * System Throughput: The number of queries the system can process per second, a key measure of scalability.
* Robustness Metrics:
   * SLO Attainment Rate: The percentage of queries that are completed within their target deadline. This is the ultimate measure of the serving infrastructure's effectiveness.1


4.3. The Next Frontier


While this roadmap delivers a state-of-the-art agent, the field is rapidly evolving. The modular architecture is designed to accommodate future enhancements. Key areas for future work include:
* Multi-Dialect Fluency: The current agent is typically tested on a primary SQL dialect. Future work should focus on explicitly training or prompting the agent to be fluent in multiple dialects (e.g., Snowflake, BigQuery, T-SQL, PL/SQL), correctly handling their unique syntax, functions, and data types.
* Complex Transformations: The agent's capabilities can be extended beyond SELECT queries to handle more complex data manipulation (e.g., INSERT, UPDATE, DELETE), analytical functions (e.g., window functions), and multi-step data transformations that are common in ETL and business intelligence workflows.
* Human-in-the-Loop Integration: For the most critical or ambiguous queries where the agent's confidence is low, the system should include a workflow to escalate the query to a human expert. The agent's best-effort SQL, along with an automatically generated natural language explanation of the query (using SQL-to-Text techniques 1), can be presented to the human for verification or refinement. This creates a powerful collaborative tool that combines the speed of automation with the reliability of human oversight.


Conclusion: Delivering a Production-Ready Text-to-SQL Agent


This document has laid out a comprehensive and actionable 14-day roadmap for constructing a world-class Text-to-SQL agent. By synthesizing the most impactful and theoretically sound techniques from leading research, this plan provides a clear, logical path from foundational concepts to a production-ready system. The proposed agent is designed to be modular, robust, and performant, addressing the multifaceted challenges of real-world database interaction.
The final agent envisioned by this roadmap is not merely a code generator; it is a comprehensive reasoning system. It understands the database's semantics through a deeply enriched, automatically constructed knowledge layer. It generates diverse and high-quality solutions through multiple, parallel reasoning paths. It demonstrates sound judgment by enforcing consensus and intelligently exploring the database to resolve ambiguity. Finally, it is engineered to perform reliably and efficiently at scale through a workflow-aware serving infrastructure. By following this blueprint, a technical team can confidently build and deploy a Text-to-SQL agent that stands at the forefront of the field, capable of democratizing data access and powering the next generation of data-driven applications.
Works cited
1. Optimizing Inference Requests for Text2SQL.pdf